{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6FnuII0duqiuSRbXpRv30",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fadhlin11/midterm_5999222021/blob/main/Midterm_Spark_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNXLozK2TJkB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo apt-get update --fix-missing\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "#!wget -q https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "!mv spark-3.0.0-bin-hadoop3.2.tgz sparkkk\n",
        "!tar xf sparkkk\n",
        "!pip install -q findspark\n",
        "     \n",
        "\n",
        "#pip install spark\n",
        "     \n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('FileStream') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.streaming import StreamingContext"
      ],
      "metadata": {
        "id": "HmYOqb96UGrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local\").appName(\"FileStream\").getOrCreate()"
      ],
      "metadata": {
        "id": "uFfRJTRDVNm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder_path = \"5999222021\" \n",
        "output_folder_path = \"5999222021\""
      ],
      "metadata": {
        "id": "IhkPM-47WeW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = StreamingContext(spark, 1)"
      ],
      "metadata": {
        "id": "dvcM8Up-WnAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream = ss.textFileStream(input_folder_path)"
      ],
      "metadata": {
        "id": "0v6V0ItOYKaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the processing logic for the stream\n",
        "def process_file(rdd):\n",
        "    # Filter out any empty lines\n",
        "    filtered_rdd = rdd.filter(lambda x: len(x) > 0)\n",
        "    \n",
        "    # Perform desired processing on the RDD\n",
        "    # For example, count the number of lines\n",
        "    count = filtered_rdd.count()\n",
        "    \n",
        "    # Save the result to an output file\n",
        "    output_file_path = f\"{output_folder_path}/result.txt\"\n",
        "    filtered_rdd.coalesce(1).saveAsTextFile(output_file_path)\n",
        "    \n",
        "    # Print the result\n",
        "    print(f\"Number of lines in file: {count}\")\n",
        "    print(f\"Result saved to: {output_file_path}\")"
      ],
      "metadata": {
        "id": "5keuZ7GuYUU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream.foreachRDD(process_file)"
      ],
      "metadata": {
        "id": "HSMql_P9Y6He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start streaming\n",
        "ss.start()"
      ],
      "metadata": {
        "id": "A5-WEpMfrQ5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2lVHli-rUEy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}